# -*- coding: utf-8 -*-
"""Phishing Websites Classification (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iV3LguWRPJQZuEB0I7O_QkvrOFQ5M9wn

# Deep Learning SP-2024: FINAL PROJECT

Project Name: FeedForward Classification (SimpleNN) of 'Phishing Websites'.

# Introduction

This project aims to leverage deep learning techniques to address a specific task using a chosen dataset.
Through this endeavor, I aim to demonstrate my understanding of deep learning concepts and techniques,
while also showcasing my ability to develop and refine a deep learning model. The project will focus on
Classification task to predict whether a website is a phishing website or not.

# Dataset Selection and Task Definition

### Dataset Description
The dataset used in this project is "Phishing Websites".
Link: https://archive.ics.uci.edu/dataset/327/phishing+websites

### Task Definition
The task to be addressed with this dataset is Classifcation task that would predict whether a website is a phishing website or not.
The evaluation metric used to assess model performance will be Accuracy.

![image.png](attachment:a0556852-659e-4525-8a27-93756ff81e29.png)
"""

pip install liac-arff

"""# Implementing Simple Neural Network

# Importing Libraries:
"""

import pandas as pd
from scipy.io import arff
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

"""# Loading Dataset"""

torch.manual_seed(42)

data, meta = arff.loadarff('Training Dataset.arff')
df = pd.DataFrame(data)

"""# Displaying the Dataset:"""

df.head()

"""# Correlation Heatmap"""

plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, fmt=".2f", cmap='vlag')
plt.show()

"""# Data Preprocessing:"""

categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

# OnehotEncoding
preprocessor = ColumnTransformer(
    transformers=[('cat', OneHotEncoder(), categorical_cols)],
    remainder='passthrough'
)
X_processed = preprocessor.fit_transform(df)

"""# Preparing Data:"""

X = X_processed[:, :-1]
y = X_processed[:, -1]

# Setting data for Tensor
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.long)

"""# Data Cleaning:"""

# Checking Missing Values
print("Missing values:")
print(df.isnull().sum())

# Checking for Duplicates
duplicate_rows = df[df.duplicated()]
print("Duplicate entries:")
print(duplicate_rows)

# Removing Missing and Duplicate Valuess
df = df.dropna()
df = df.drop_duplicates()

"""# Defining the Model:"""

class SimpleNN(nn.Module):
    def __init__(self, input_dim):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 2)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

input_dim = X.shape[1]
model = SimpleNN(input_dim)

"""# Training the Model:"""

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Initializing Number of Epochs and Runs
num_epochs = 10
num_runs = 10
avg_accuracy = 0
all_val_losses = []

"""# Training Loop:"""

for run in range(num_runs):
    # Splitting the data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=run)
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.long)
    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
    y_val_tensor = torch.tensor(y_val, dtype=torch.long)

    val_losses = []
    for epoch in range(num_epochs):
        # Training step
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train_tensor)
        loss = criterion(outputs, y_train_tensor)
        loss.backward()
        optimizer.step()

        # Validation step
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_tensor)
            val_loss = criterion(val_outputs, y_val_tensor)
            val_losses.append(val_loss.item())  # Storing loss for each epoch

        print(f'Epoch [{run+1}/{num_runs}], Epoch [{epoch+1}/{num_epochs}], Val Loss: {val_loss.item():.4f}')

    all_val_losses.append(val_losses)  # Store loss for each run

    # calculating accuracy
    with torch.no_grad():
        val_outputs = model(X_val_tensor)
        _, predicted = torch.max(val_outputs, 1)
        accuracy = accuracy_score(y_val_tensor.numpy(), predicted.numpy())
    avg_accuracy += accuracy

# Calculating Average Accuracy
avg_accuracy /= num_runs

"""# Result:"""

print(f'Average Overall Accuracy on Validation Set: {avg_accuracy * 100:.2f}%')

"""# Plotting Result:"""

plt.figure(figsize=(10, 6))
for i, val_loss_run in enumerate(all_val_losses):
    plt.plot(range(1, num_epochs + 1), val_loss_run, label=f'Run {i+1}')
plt.xlabel('Epochs')
plt.ylabel('Validation Loss')
plt.title('Validation Loss over Epochs')
plt.legend()
plt.grid(True)
plt.show()

"""# Hyperparameter Tuning:

# Importing Libraries:
"""

import pandas as pd
from scipy.io import arff
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score

"""# Loading Dataset:"""

data, meta = arff.loadarff('Training Dataset.arff')
df = pd.DataFrame(data)

"""# Data Preprocessing:"""

categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
preprocessor = ColumnTransformer(
    transformers=[('cat', OneHotEncoder(), categorical_cols)],
    remainder='passthrough'
)
X_processed = preprocessor.fit_transform(df)
X = X_processed[:, :-1]
y = X_processed[:, -1]

"""# Splitting Dataset:"""

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.long)

"""# Defining the Model:"""

class SimpleNN(nn.Module):
    def __init__(self, input_dim):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 32)
        self.fc4 = nn.Linear(32, 2)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)
        x = torch.relu(self.fc3(x))
        x = self.dropout(x)
        x = self.fc4(x)
        return x

"""# Hyperparameter Tuning:"""

learning_rates = [0.01, 0.001, 0.0001]
num_epochs_options = [5, 10, 15, 20]
best_accuracy = 0
best_lr = 0
best_epoch = 0
for lr in learning_rates:
    for num_epochs in num_epochs_options:
        model = SimpleNN(X.shape[1])
        optimizer = optim.Adam(model.parameters(), lr=lr)
        criterion = nn.CrossEntropyLoss()
        for epoch in range(num_epochs):
            model.train()
            optimizer.zero_grad()
            outputs = model(X_train_tensor)
            loss = criterion(outputs, y_train_tensor)
            loss.backward()
            optimizer.step()
            model.eval()
            with torch.no_grad():
                val_outputs = model(X_val_tensor)
                _, predicted = torch.max(val_outputs, 1)
                val_accuracy = accuracy_score(y_val_tensor.numpy(), predicted.numpy())
            if val_accuracy > best_accuracy:
                best_accuracy = val_accuracy
                best_lr = lr
                best_epoch = num_epochs
        print(f'LR: {lr}, Epochs: {num_epochs}, Val Accuracy: {val_accuracy:.4f}')

"""# Results:"""

print(f'Best Accuracy: {best_accuracy * 100:.2f}%, Best Learning Rate: {best_lr}, Best Epoch: {best_epoch}')

"""# Plotting the Result:"""

import matplotlib.pyplot as plt

# Storing loss values
train_losses = []
val_losses = []

# Training
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()

    train_losses.append(loss.item())

    # Validation
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val_tensor)
        val_loss = criterion(val_outputs, y_val_tensor)
        val_losses.append(val_loss.item())


# Plotting
plt.figure(figsize=(10, 6))
plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')
plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss over Epochs')
plt.legend()
plt.grid(True)
plt.show()

"""# Conclusion:

In conclusion, this project successfully leveraged deep learning techniques to address the task of Classification of dataset to check wether a webshite is a phishing website or not.
The trained model achieved 93.14% Accuracy on the test set, demonstrating an excellent Model.

!~~~~ END ~~~~!
"""